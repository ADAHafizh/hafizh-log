---
title: 'Capstone Seminar | Prototype 7'
description: 'Recognizing Human Parts using Azure Kinect'
date: 2025-11-12
tags: ['capstone']
image: './bg-capstone.png'
authors: ['adahafizh']
---

# Following Last week

For this week, I specifically focused on configuring the depth camera (Kinect Azure) so that it can track certain parameters and use it in the visuals. Continuing the previous week visual: *watercolor drawing*, I used Azure's body part recognition and selected `hands` to be the element that I track. TL:DR, what I am doing is basically as follows:

<div align="center">
    <figure>
        <img src="https://i.imgur.com/i42oos6.png" alt=""/>
        <figcaption> Simplified Diagram </figcaption>
    </figure>
    <figure>
        <img src="https://i.imgur.com/FNmB5XR.png" alt=""/>
        <figcaption> TouchDesigner Network Nodes </figcaption>
    </figure>             
</div> 

> Essentially, how the visual works is that I am using a circle shape to be used as a pointer. This circle would then apply paint when it detects movement from the hand to the screen. The paint would then disperse and fade after some time, mimicking painting using watercolor. 

# How It Went

Apparently, there were a lot of things to consider when selecting certain elements from Kinect Azure. What I mean by this is that the camera by default has built-in recognition system for the entire skeleton plus all things accompanying it: hands, finger, legs, pelvis, head, etc. Plus, it is split between three coordinate systems: `x,y,z` meaning that I have to transform 3D information to 2D and then applying to my visual. 

<div align="center">
    <figure>
        <img src="https://i.imgur.com/EWgVFR5.gif" alt=""/>
        <figcaption> Wrong element plugging causing left hand to go up and down </figcaption>
    </figure>
    <figure>
        <img src="https://i.imgur.com/03RQ2tB.gif" alt=""/>
        <figcaption> Linking coordinate position of my hand to the circle </figcaption>
    </figure>
    <figure>
        <img src="https://i.imgur.com/WYeg8fk.gif" alt=""/>
        <figcaption> Testing both hand recognition while sitting </figcaption>
    </figure>             
</div> 

# Challenges and Worries

One of the most challenging part about this prototype was simply `understanding the amounts of information Kinect Azure spews out,` and processing what to do with the copius informations. I had to scour through YouTube tutorials to get a glimpse and only after experimenting for three hours that I "kind of" understood how this specific element on body tracking works. 

<div align="center">
    <figure>
        <img src="https://i.imgur.com/DgTsujC.png" alt=""/>
        <figcaption> Every information Kinect Azure tracks (on player index) </figcaption>
    </figure>
</div>

Second, because I was sick throughout the weekends, I wasn't able to regroup with Yaakulya to join our design and see how things look, feel, and sound. But hearing from his side and how things have been working out, all gonna be gucci gucci. 

Given that we were only able to get equipment booking fully checked out this week, we were not able to hand the projector (and supposedly cameras). Thankfully, Professor Nimrah approved it, so it's out of the way for now. `Meaning for next week, our biggest hurdle will be figuring out the floor planning and equipment hanging.` This worries me a lot because once things are properly secured, every element has to be reconfigured so that it works. **My biggest fear is that the tracking from the top is very different than front.** Plus, *how do we hide ALL THIS TECH?!*

> But like the old saying, blame the player not the game. Adapt, improve, overcome~

### External Resources
[Custom Depth Map from Kinect & RealSense Point Clouds in TouchDesigner](https://www.youtube.com/watch?v=p-lhmCMxn2g)
[Kinect 2 vs Kinect Azure in TouchDesigner](https://youtu.be/Fv35hjl4qRI?si=-jqJ9HsJ93704s-d)
